# ‚úÖ Minimal LLaMA3 Tool-Calling Agent (No LangChain Agent)
# Uses Ollama (LLaMA3) to parse natural language into JSON for tool execution

import json
import subprocess
import sys
from pathlib import Path
from PyPDF2 import PdfReader

import requests
LLM_URL     = "http://localhost:11434"
# === STEP 1: Define local tools ===
def add_numbers(numbers):
    print("[DEBUG] add_numbers() called")
    return sum(numbers)

def subtract(a, b):
    print("[DEBUG] subtract() called")
    return a - b

def multiply(numbers):
    print("[DEBUG] multiply() called")
    result = 1
    for n in numbers:
        result *= n
    return result

def divide(a, b):
    print("[DEBUG] divide() called")
    if b == 0:
        return "Error: Division by zero"
    return a / b

def get_weather(city):
    print(f"[DEBUG] get_weather() called with city={city}")
    return f"The weather in {city} is 25¬∞C and sunny (hardcoded)"

def analyze_document(path):
    print(f"[DEBUG] analyze_document() called with path={path}")
    file_path = Path(path)

    if not file_path.exists():
        return f"File not found: {path}"
        
    # Read text based on file type
    if file_path.suffix.lower() == ".txt":
        text = file_path.read_text(encoding="utf-8")
    elif file_path.suffix.lower() == ".pdf":
        reader = PdfReader(str(file_path))
        text = "\n".join(page.extract_text() or "" for page in reader.pages)
    else:
        return "Unsupported file type. Only .txt and .pdf are supported."

    # Send summary request to LLaMA3
    print("[INFO] Sending document content for summarization...")
    prompt = f"Please summarize or analyze the following document content:\n{text[:4000]}"  # Truncate for safety
    response = call_llama3(prompt)
    return response.strip()


# === STEP 2: Define tool registry ===
tool_registry = {
    "add_numbers": add_numbers,
    "subtract": subtract,
    "multiply": multiply,
    "divide": divide,
    "get_weather": get_weather,
    "analyze_document": analyze_document
}

# === STEP 3: Use LLaMA3 via Ollama to parse the user's intent ===
def call_llama3(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "llama3",
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

# === STEP 4: Parse intent and execute tools manually ===
def process_input(user_query):
    print("\n[INFO] [process_input] Sending prompt to LLaMA3...")
    system_prompt = (
        "You are an AI tool-calling assistant."
        " Read the user's query and return an output in JSON object in the format: {\"tool\": tool_name, \"args\": arguments}."
        "\nAvailable tools are:"
        " add_numbers(numbers: list of numbers) : returns the result of addition"
        " subtract(a: number, b: number) : returns the result of substraction"
        " multiply(numbers: list of numbers) : returns the result of multiplication"
        " divide(a: number, b: number) : returns the result of division"
        " get_weather(city: string) : returns the weather of the city passed as a parameter."
        " analyze_document(path: string) : analyzes or summarizes a text or PDF document from the specified file path."
        "\nIf there is no available tool for the respective user input, then just return { \"tool\": null, \"args\": { \"query\": \"...\" } }"
        "\nONLY return a valid JSON. No explanation, no markdown."
    )

    full_prompt = system_prompt + f"\nUser: {user_query}\n"
    output = call_llama3(full_prompt)

    try:
        print(f"[DEBUG] LLaMA3 output: {output}")
        tool_call = json.loads(output)
        tool_name = tool_call["tool"]
        args = tool_call["args"]

        if tool_name in tool_registry:
            result = tool_registry[tool_name](**args)
            print("‚úÖ Tool '{tool_name}' returned: {result}")
            return f"{result}"

        elif tool_name is None:
            # Fallback to direct LLM response
            print("[INFO] No tool used. Asking LLaMA3 directly for response...")
            response = call_llama3(args.get("query", user_query))
            return f"{response}"

        else:
            return f"‚ùå Unknown tool: {tool_name}"

    except json.JSONDecodeError:
        return "‚ùå Invalid JSON from LLaMA3. Falling back to chat mode:\n" + call_llama3(user_query)

    except Exception as e:
        return f"‚ùå Error while executing tool: {e}"


# === CLI Entry Point ===
if __name__ == "__main__":
    print("ü§ñ LLaMA3 Tool Executor (no LangChain)")
    while True:
        user_input = input("\nüß† Your query (or 'exit'): ")
        if user_input.strip().lower() in ["exit", "quit"]:
            break
        process_input(user_input)

